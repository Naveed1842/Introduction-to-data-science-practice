{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key part of KNN is the **choice of K**. Each new point is predicted by its k nearest neighbor in the training set. As k increases this variability is reduced. But if we increase k too much, then we no longer follow the true boundary line we observe high bias. This the nature of the Bias-Variance Tradeoff. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing Bias and Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias and variance are equally important and one should not be improved at an excessive expnese to the other. \n",
    "\n",
    "## Bagging and Resampling\n",
    "\n",
    "In Bagging, numerous replicates of the original data set are created using random Eselection with replacement. Each derivative data set is then used to construct a new model and the models are gathered together into an ensemblem. Ti make a prediction,all of the models in the ensemble are polled and their results are averaged. \n",
    "\n",
    "One powerful modeling algorithm that makes good use of bagging is Random Forest. Random Forests works by trainning numerous decision trees each based on a different resampling of the original training data. In Random Forests, the bias of the full model is equivalent to the bias of a single decision tree(which iteself has high variance). By creating many of these trees, in effect a 'forest'., and then averaging them the variance of the final model can greatly reduced over that of a single tree. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing KNN with other models\n",
    "\n",
    "**Advantages of KNN:**\n",
    "\n",
    "- Simple to understand and explain\n",
    "- Model training is fast\n",
    "- Can be used for classification and regression\n",
    "\n",
    "**Disadvantages of KNN:**\n",
    "\n",
    "- Must store all of the training data\n",
    "- Prediciton phase can be slow when n is large\n",
    "- Sensitive to irrelevant features\n",
    "- Accuracy is (generally) not competitive with the best supervised learning methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
